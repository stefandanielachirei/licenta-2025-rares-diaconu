<<<<<<< HEAD
Text Summarization: Pe partea asta am gasit "facebook/bart-large-cnn" model, este cel mai 		    	    mare si cel mai folosit si cred ca vom merge cu el inainte. Din ce am 		    	    testat merge si cu texte mai lungi pe care de exemplu modelul pentru 		    	    sentiment analysis nu il poate analiza.
		    https://huggingface.co/facebook/bart-large-cnn
Sentiment Analysis: Am gasit modelul bert-base-multilingual-uncased-sentiment care reuseste 		    sa analizeze sentimentele de la very negative la very positive avand 5 		    parametrii la iesire. 
		    Problema este ca din cele pe care le-am gasit niciunul nu analizeaza pe 		    mai mult de 512 de caractere/size of tensor b si din aceasta cauza un 		    review trebuie analizat pe bucati si facut o medie. Am facut o functie 		    care imparte textul in bucati mai mici de 512 caractere si am reusit sa 		    analizez in acest mod si vom merge mai departe cu modelul si functia 		    facuta.
		    https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment
Text similarity: Am gasit modelul sentence-transformers/all-MiniLM-L6-v2 care reuseste sa 			analizeze similaritatea dintre 2 texte destul de mari si primesc un tensor 			care conține embedding-uri pentru fiecare propoziție pe care am introdus-o. 			Apoi am analizat-o cu similaritatea cosinusului si am primit o valoare prin 			care se deduce cat de similare sunt textele.
		https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

Dataset : vom folosi goodbooks 10k de la kaggle, sunt destule carti si avem ratings, book-	  tags, books, tags si to_read de unde sa alegem doar ca trebuie facut cont

Model de antrenat: M-am interesat si a trebuit sa aleg un model encoder-decoder care este antrenat pentru mai multe task-uri pe text si care interactioneaza cu utilizatorul si am gasit modelul https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct. Acest model are ca prima functie generarea de text dar poate fi antrenat si pe sumarizare desi nu a fost antrenat pe asta pana acum. Pe partea de documentatie am gasit acest link https://huggingface.co/docs/transformers/en/tasks/summarization ce explica cum sa fine-tunam orice model si cu tensorflow si cu PyTorch cu care preferam. Mai sunt si aceste link-uri pentru fine-tuning(cod, tutoriale si documentatii) https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization
https://gitee.com/mecount/fairseq/blob/master/examples/bart/README.summarization.md
https://sparknlp.org/2023/04/09/bart_large_cnn_en.html
https://www.youtube.com/watch?v=CDmPBsZ09wg - tutorial antrenare model
Am incercat sa scriu ceva cod pentru antrenare dar mai greu sa adaug cartile in colab insa am scris ceva cod dar am vrut sa astept poate le voi da upload la carti direct pe serverul cu cele 4 t100 si ma apuc direct acolo. Asta ar fi primul pas sa am cartile sa le sortez si apoi sa tokenizez si sa preprocesez datele si apoi sa antrenez modelul.
Am citit si despre cosine similarity un pic si am mai aprofundat si vom folosi la sentence similarity cand vom ajunge acolo. Am folosit aceste link-uri: 
https://www.sciencedirect.com/topics/computer-science/cosine-similarity
https://www.geeksforgeeks.org/cosine-similarity/
https://www.datastax.com/guides/what-is-cosine-similarity



Am configurat env-ul pe HPC cu tot ce trebuie, am incarcat cartile acum trebuie doar sa le aranjez ca sa le folosesc si cele 3 script-uri pentru modelele pe care le vom importa.

https://www.kaggle.com/datasets/zygmunt/goodbooks-10k?select=tags.csv
https://www.tensorflow.org/datasets/catalog/booksum

Model deployment

- link-uri de unde am citit si mai jos sunt ideile principale : 
	- https://builtin.com/machine-learning/model-deployment
	- https://www.kdnuggets.com/deploying-machine-learning-models-a-step-by-step-tutorial
	- https://www.qwak.com/post/what-does-it-take-to-deploy-ml-models-in-production
	- https://www.projectpro.io/article/machine-learning-model-deployment/872
	- https://aws.amazon.com/what-is/mlops/
	- https://ml-ops.org/
	- https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
	- https://www.youtube.com/watch?app=desktop&v=rgr_aCg-338
	- https://www.youtube.com/watch?v=Gs15V79cauo
	- https://www.youtube.com/watch?v=vA0C0k72-b4

 - data preprocessing -> model training -> model testing
					<- 
					-> model packaging -> model testing -> model deployment
- ML model deployment is just as important as ML model development.  -> pentru ca daca nu il si folosim degeaba am antrenat acel model

model.save("trained_model") in tensorflow
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, "trained_model.pth") in torch

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model.save_pretrained("model_directory")
tokenizer.save_pretrained("model_directory") pentru cele care folosesc transformers

Cred ca pentru doar un proiect de licenta putem face totul pe hpc unde sa il antrenam si apoi sa il folosim tot acolo, luandu-l din director. Trebuie ca in director sa avem aceste fisiere: 
	- config.json
	- pytorch_model.bin/tf_model.h5
	- tokenizer_config.json
	- vocab.txt/tokenizer.json
	- special_tokens_map.json
Si il vom folosi cu cereri api din fastapi(schimbat de la flask) direct din director.
Daca vrem sa il folosim si in alte locuri nu doar pe HPC, va trebuie sa folosim docker pentru portabilitate si ajuta la pastrarea dependentelor de pe un pc pe altul. Cred ca cel mai bine este sa il punem in docker direct si apoi sa il folosim de oriunde dupa ce terminam de antrenat modelul.


=======
Text Summarization: Pe partea asta am gasit "facebook/bart-large-cnn" model, este cel mai 		    	    mare si cel mai folosit si cred ca vom merge cu el inainte. Din ce am 		    	    testat merge si cu texte mai lungi pe care de exemplu modelul pentru 		    	    sentiment analysis nu il poate analiza.
		    https://huggingface.co/facebook/bart-large-cnn
Sentiment Analysis: Am gasit modelul bert-base-multilingual-uncased-sentiment care reuseste 		    sa analizeze sentimentele de la very negative la very positive avand 5 		    parametrii la iesire. 
		    Problema este ca din cele pe care le-am gasit niciunul nu analizeaza pe 		    mai mult de 512 de caractere/size of tensor b si din aceasta cauza un 		    review trebuie analizat pe bucati si facut o medie. Am facut o functie 		    care imparte textul in bucati mai mici de 512 caractere si am reusit sa 		    analizez in acest mod si vom merge mai departe cu modelul si functia 		    facuta.
		    https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment
Text similarity: Am gasit modelul sentence-transformers/all-MiniLM-L6-v2 care reuseste sa 			analizeze similaritatea dintre 2 texte destul de mari si primesc un tensor 			care conține embedding-uri pentru fiecare propoziție pe care am introdus-o. 			Apoi am analizat-o cu similaritatea cosinusului si am primit o valoare prin 			care se deduce cat de similare sunt textele.
		https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

Dataset : vom folosi goodbooks 10k de la kaggle, sunt destule carti si avem ratings, book-	  tags, books, tags si to_read de unde sa alegem doar ca trebuie facut cont

Model de antrenat: M-am interesat si a trebuit sa aleg un model encoder-decoder care este antrenat pentru mai multe task-uri pe text si care interactioneaza cu utilizatorul si am gasit modelul https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct. Acest model are ca prima functie generarea de text dar poate fi antrenat si pe sumarizare desi nu a fost antrenat pe asta pana acum. Pe partea de documentatie am gasit acest link https://huggingface.co/docs/transformers/en/tasks/summarization ce explica cum sa fine-tunam orice model si cu tensorflow si cu PyTorch cu care preferam. Mai sunt si aceste link-uri pentru fine-tuning(cod, tutoriale si documentatii) https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization
https://gitee.com/mecount/fairseq/blob/master/examples/bart/README.summarization.md
https://sparknlp.org/2023/04/09/bart_large_cnn_en.html
https://www.youtube.com/watch?v=CDmPBsZ09wg - tutorial antrenare model
Am incercat sa scriu ceva cod pentru antrenare dar mai greu sa adaug cartile in colab insa am scris ceva cod dar am vrut sa astept poate le voi da upload la carti direct pe serverul cu cele 4 t100 si ma apuc direct acolo. Asta ar fi primul pas sa am cartile sa le sortez si apoi sa tokenizez si sa preprocesez datele si apoi sa antrenez modelul.
Am citit si despre cosine similarity un pic si am mai aprofundat si vom folosi la sentence similarity cand vom ajunge acolo. Am folosit aceste link-uri: 
https://www.sciencedirect.com/topics/computer-science/cosine-similarity
https://www.geeksforgeeks.org/cosine-similarity/
https://www.datastax.com/guides/what-is-cosine-similarity



Am configurat env-ul pe HPC cu tot ce trebuie, am incarcat cartile acum trebuie doar sa le aranjez ca sa le folosesc si cele 3 script-uri pentru modelele pe care le vom importa.

https://www.kaggle.com/datasets/zygmunt/goodbooks-10k?select=tags.csv
https://www.tensorflow.org/datasets/catalog/booksum

Model deployment

- link-uri de unde am citit si mai jos sunt ideile principale : 
	- https://builtin.com/machine-learning/model-deployment
	- https://www.kdnuggets.com/deploying-machine-learning-models-a-step-by-step-tutorial
	- https://www.qwak.com/post/what-does-it-take-to-deploy-ml-models-in-production
	- https://www.projectpro.io/article/machine-learning-model-deployment/872
	- https://aws.amazon.com/what-is/mlops/
	- https://ml-ops.org/
	- https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
	- https://www.youtube.com/watch?app=desktop&v=rgr_aCg-338
	- https://www.youtube.com/watch?v=Gs15V79cauo
	- https://www.youtube.com/watch?v=vA0C0k72-b4

 - data preprocessing -> model training -> model testing
					<- 
					-> model packaging -> model testing -> model deployment
- ML model deployment is just as important as ML model development.  -> pentru ca daca nu il si folosim degeaba am antrenat acel model

model.save("trained_model") in tensorflow
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, "trained_model.pth") in torch

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model.save_pretrained("model_directory")
tokenizer.save_pretrained("model_directory") pentru cele care folosesc transformers

Cred ca pentru doar un proiect de licenta putem face totul pe hpc unde sa il antrenam si apoi sa il folosim tot acolo, luandu-l din director. Trebuie ca in director sa avem aceste fisiere: 
	- config.json
	- pytorch_model.bin/tf_model.h5
	- tokenizer_config.json
	- vocab.txt/tokenizer.json
	- special_tokens_map.json
Si il vom folosi cu cereri api din fastapi(schimbat de la flask) direct din director.
Daca vrem sa il folosim si in alte locuri nu doar pe HPC, va trebuie sa folosim docker pentru portabilitate si ajuta la pastrarea dependentelor de pe un pc pe altul. Cred ca cel mai bine este sa il punem in docker direct si apoi sa il folosim de oriunde dupa ce terminam de antrenat modelul.


>>>>>>> dbab91eacac44fef15f6b5bb7f712bd5254f5d10

care este limita de input token la Llama-3.1-8B-Instruct(2048) si la Qwen2.5-1.5B-Instruct de gasit si daca este mai mic, sa schimbam si aici si in raport

Am analizat limitele de token-uri de intrare de la fiecare model din top de pe hugging face de la text generation si am realizat ca l-am ales pe cel mai bun de prima data. Qwen2.5-1.5B-Instruct are suport pentru texte foarte lungi de pana la 32768 tokeni ceea ce il face ideal pentru task-ul de sumarizare pe dataset-ul booksum. Vom ramane cu acesta si il vom face sa mearga cumva.


Ca dataset, nu vom mai folosi booksum pentru ca este destul de mic si destul de greu de incarcat. Vom folosi cu functia load_dataset din datasets dataset = load_dataset("cnn_dailymail", "3.0.0")
si vom antrena cat de multe epoci se poate. Programul ruleaza desi destul de greu si primesc eroarea asta dupa ce ruleaza cam 1h torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 39.70 GiB total capacity; 15.78 GiB already allocated; 35.19 MiB free; 16.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|          | 0/71778 [00:01<?, ?it/s] si nu prea stiu cum sa o rezolv, am micsorat nr de input tokens si alte cateva solutii si nu reusesc. Am modificat si in raport unde am scris de dataset-ul de la booksum si am scris de cnn_dailymail 3.0.0

https://www.datacamp.com/tutorial/loss-function-in-machine-learning
https://blog.dailydoseofds.com/p/an-algorithm-wise-summary-of-loss
https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb
https://cyborgcodes.medium.com/what-is-early-stopping-in-deep-learning-eeb1e710a3cf
https://developers.google.com/machine-learning/crash-course/overfitting/overfitting
https://huggingface.co/meta-llama/Llama-3.2-1B
https://www.datacamp.com/blog/classification-machine-learning
https://www.ibm.com/topics/natural-language-processing
https://neptune.ai/blog/tips-to-train-nlp-models
https://www.linkedin.com/advice/0/how-do-you-train-natural-language-processing
https://www.youtube.com/watch?v=pK8u4QfdLx0
https://chatgpt.com/share/6758add8-bc38-8009-aa0c-6434f3ac367e

Astea sunt rezultatele de la antrenare fara LoRA

Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13368/13368 [00:34<00:00, 384.45 examples/s]
Epoch 1/10
Training:   0%|                                                                                                                                                                   | 0/12500 [00:00<?, ?it/s]C:\Users\Operator\.conda\envs\Machine_Learning_Licenta_AC\lib\site-packages\transformers\models\llama\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Training:  50%|██████████████████████████████████████████████████████████████████████▏ 

1, Training Loss: 10.334211371421814
Validation:   0%|                                                                                                                                                                  | 0/3342 [00:00<?, ?it/s]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|                                                                                                                                         | 1/3342 [00:08<7:41:48,  8.29s/it, val_loss=7.71]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|                                                                                                                                         | 2/3342 [00:09<3:53:08,  4.19s/it, val_loss=7.46]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|                                                                                                                                         | 3/3342 [00:17<5:19:39,  5.74s/it, val_loss=7.71]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|▏                                                                                                                                        | 4/3342 [00:24<6:02:07,  6.51s/it, val_loss=7.36]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|▏                                                                                                                                        | 5/3342 [00:32<6:23:32,  6.90s/it, val_loss=7.36]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|▏                                                                                                                                        | 6/3342 [00:40<6:36:50,  7.14s/it, val_loss=7.35]Setting pad_token_id to eos_token_id:None for open-end generation.
Validation:   0%|▎                                                                                                                                        | 7/3342 [00:47<6:46:10,  7.31s/it, val_loss=8.88]Setting pad_token_id to eos_token_id:None for open-end generation.

Validation:  21%|████████████████████████████▍                                                                                                        | 716/3342 [1:28:57<5:32:46,  7.60s/it, val_loss=13.3]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Validation:  21%|████████████████████████████▌                                                                                                        | 717/3342 [1:29:05<5:32:40,  7.60s/it, val_loss=13.7]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Validation:  21%|████████████████████████████▌                                                                                                        | 718/3342 [1:29:12<5:32:29,  7.60s/it, val_loss=12.7]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Validation:  22%|████████████████████████████▌                                                                                                        | 719/3342 [1:29:20<5:33:37,  7.63s/it, val_loss=11.4]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Validation:  22%|████████████████████████████▋                                                                                                        | 720/3342 [1:29:28<5:34:10,  7.65s/it, val_loss=13.7]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

Astea sunt rezultatele de la antrenare cu LoRA

trainable params: 1,703,936 || all params: 1,237,520,384 || trainable%: 0.1377
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11490/11490 [00:27<00:00, 424.12 examples/s]
Epoch 1/10
Training:   0%|                                                                                                                                                                   | 0/12500 [00:00<?, ?it/s]C:\Users\Operator\.conda\envs\Machine_Learning_Licenta_AC\lib\site-packages\transformers\models\llama\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Epoch 1, Training Loss: 11.777944088668823
Epoch 1, Validation Loss: 11.646943058673669
Epoch 1, ROUGE Score: {'rouge1': 0.14759450271936947, 'rouge2': 0.08367271742006074, 'rougeL': 0.10716193805811874, 'rougeLsum': 0.13035568959330818}                


